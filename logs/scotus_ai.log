2025-07-15 09:25:59 | INFO     | __main__:demo_lr_scheduler:24 | ðŸŽ¯ SCOTUS AI Enhanced LR Scheduler Demo
2025-07-15 09:25:59 | INFO     | __main__:demo_lr_scheduler:25 | ============================================================
2025-07-15 09:26:58 | INFO     | __main__:demo_lr_scheduler:24 | ðŸŽ¯ SCOTUS AI Enhanced LR Scheduler Demo
2025-07-15 09:26:58 | INFO     | __main__:demo_lr_scheduler:25 | ============================================================
2025-07-15 09:27:01 | INFO     | __main__:demo_lr_scheduler:39 | ðŸ“Š LR Scheduler Configuration:
2025-07-15 09:27:01 | INFO     | __main__:demo_lr_scheduler:40 |    - Mode: minimize validation loss
2025-07-15 09:27:01 | INFO     | __main__:demo_lr_scheduler:41 |    - Factor: 0.5 (LR multiplier on plateau)
2025-07-15 09:27:01 | INFO     | __main__:demo_lr_scheduler:42 |    - Patience: 3 epochs
2025-07-15 09:27:01 | INFO     | __main__:demo_lr_scheduler:43 |    - Initial LR: 1.00e-03
2025-07-15 09:27:01 | INFO     | __main__:demo_lr_scheduler:44 |    - Custom logging: Enabled (will log LR reductions)
2025-07-15 09:27:01 | INFO     | __main__:demo_lr_scheduler:45 | 
2025-07-15 09:27:01 | INFO     | __main__:demo_lr_scheduler:50 | ðŸš€ Starting simulated training...
2025-07-15 09:27:01 | INFO     | __main__:demo_lr_scheduler:51 | ðŸ“‰ Validation losses will plateau to trigger LR reductions:
2025-07-15 09:27:01 | INFO     | __main__:demo_lr_scheduler:52 |    Epochs 1-4: Decreasing loss (1.0 â†’ 0.5)
2025-07-15 09:27:01 | INFO     | __main__:demo_lr_scheduler:53 |    Epochs 5-7: Plateau at 0.5 (triggers first LR reduction)
2025-07-15 09:27:01 | INFO     | __main__:demo_lr_scheduler:54 |    Epochs 8-12: Plateau at 0.4 (triggers second LR reduction)
2025-07-15 09:27:01 | INFO     | __main__:demo_lr_scheduler:55 |    Epoch 13: Improves to 0.3
2025-07-15 09:27:01 | INFO     | __main__:demo_lr_scheduler:56 | 
2025-07-15 09:27:01 | INFO     | __main__:demo_lr_scheduler:70 | Epoch  1: Val Loss = 1.0, LR = 1.00e-03
2025-07-15 09:27:01 | INFO     | __main__:demo_lr_scheduler:70 | Epoch  2: Val Loss = 0.8, LR = 1.00e-03
2025-07-15 09:27:01 | INFO     | __main__:demo_lr_scheduler:70 | Epoch  3: Val Loss = 0.6, LR = 1.00e-03
2025-07-15 09:27:01 | INFO     | __main__:demo_lr_scheduler:70 | Epoch  4: Val Loss = 0.5, LR = 1.00e-03
2025-07-15 09:27:01 | INFO     | __main__:demo_lr_scheduler:70 | Epoch  5: Val Loss = 0.5, LR = 1.00e-03
2025-07-15 09:27:01 | INFO     | __main__:demo_lr_scheduler:70 | Epoch  6: Val Loss = 0.5, LR = 1.00e-03
2025-07-15 09:27:01 | INFO     | __main__:demo_lr_scheduler:70 | Epoch  7: Val Loss = 0.5, LR = 1.00e-03
2025-07-15 09:27:01 | INFO     | __main__:demo_lr_scheduler:70 | Epoch  8: Val Loss = 0.4, LR = 1.00e-03
2025-07-15 09:27:01 | INFO     | __main__:demo_lr_scheduler:70 | Epoch  9: Val Loss = 0.4, LR = 1.00e-03
2025-07-15 09:27:01 | INFO     | __main__:demo_lr_scheduler:70 | Epoch 10: Val Loss = 0.4, LR = 1.00e-03
2025-07-15 09:27:01 | INFO     | __main__:demo_lr_scheduler:70 | Epoch 11: Val Loss = 0.4, LR = 1.00e-03
2025-07-15 09:27:01 | INFO     | __main__:demo_lr_scheduler:70 | Epoch 12: Val Loss = 0.4, LR = 5.00e-04
2025-07-15 09:27:01 | INFO     | __main__:demo_lr_scheduler:74 |    ðŸ”½ Learning rate reduced: 1.00e-03 â†’ 5.00e-04
2025-07-15 09:27:01 | INFO     | __main__:demo_lr_scheduler:70 | Epoch 13: Val Loss = 0.3, LR = 5.00e-04
2025-07-15 09:27:01 | INFO     | __main__:demo_lr_scheduler:77 | 
2025-07-15 09:27:01 | INFO     | __main__:demo_lr_scheduler:78 | ðŸ“Š LR Scheduler Summary:
2025-07-15 09:27:01 | INFO     | __main__:demo_lr_scheduler:83 |    - Initial LR: 1.00e-03
2025-07-15 09:27:01 | INFO     | __main__:demo_lr_scheduler:84 |    - Final LR: 5.00e-04
2025-07-15 09:27:01 | INFO     | __main__:demo_lr_scheduler:85 |    - Total reductions: 1
2025-07-15 09:27:01 | INFO     | __main__:demo_lr_scheduler:86 |    - LR reduction factor: 0.500
2025-07-15 09:27:01 | INFO     | __main__:demo_lr_scheduler:88 | 
2025-07-15 09:27:01 | INFO     | __main__:demo_lr_scheduler:89 | âœ… Demo completed!
2025-07-15 09:27:01 | INFO     | __main__:demo_lr_scheduler:90 | This is how the enhanced LR scheduler works in SCOTUS AI training.
2025-07-15 09:32:26 | INFO     | __main__:simulate_training_with_combined_metric:24 | ðŸŽ¯ LR Scheduler with Combined Metric Test
2025-07-15 09:32:26 | INFO     | __main__:simulate_training_with_combined_metric:25 | ============================================================
2025-07-15 09:32:30 | INFO     | __main__:simulate_training_with_combined_metric:36 | ðŸ“Š LR Scheduler Configuration:
2025-07-15 09:32:30 | INFO     | __main__:simulate_training_with_combined_metric:37 |    - Mode: minimize combined metric (Val Loss + (1-F1))/2
2025-07-15 09:32:30 | INFO     | __main__:simulate_training_with_combined_metric:38 |    - Factor: 0.5 (LR multiplier on plateau)
2025-07-15 09:32:30 | INFO     | __main__:simulate_training_with_combined_metric:39 |    - Patience: 3 epochs
2025-07-15 09:32:30 | INFO     | __main__:simulate_training_with_combined_metric:40 |    - Initial LR: 1.00e-03
2025-07-15 09:32:30 | INFO     | __main__:simulate_training_with_combined_metric:41 |    - Metric: Combined metric for consistency
2025-07-15 09:32:30 | INFO     | __main__:simulate_training_with_combined_metric:42 | 
2025-07-15 09:32:30 | INFO     | __main__:simulate_training_with_combined_metric:59 | ðŸš€ Starting simulated training...
2025-07-15 09:32:30 | INFO     | __main__:simulate_training_with_combined_metric:60 | ðŸ“ˆ Combined metric = (Val Loss + (1-F1))/2
2025-07-15 09:32:30 | INFO     | __main__:simulate_training_with_combined_metric:61 | ðŸ“‰ Epochs 4-7 will plateau to trigger LR reduction
2025-07-15 09:32:30 | INFO     | __main__:simulate_training_with_combined_metric:62 | 
2025-07-15 09:32:31 | INFO     | __main__:simulate_training_with_combined_metric:82 | Epoch 1: Val Loss = 0.8, F1-Macro = 0.74, Combined = 0.53, LR = 1.00e-03
2025-07-15 09:32:31 | INFO     | __main__:simulate_training_with_combined_metric:82 | Epoch 2: Val Loss = 0.6, F1-Macro = 1.00, Combined = 0.30, LR = 1.00e-03
2025-07-15 09:32:31 | INFO     | __main__:simulate_training_with_combined_metric:82 | Epoch 3: Val Loss = 0.5, F1-Macro = 0.84, Combined = 0.33, LR = 1.00e-03
2025-07-15 09:32:31 | INFO     | __main__:simulate_training_with_combined_metric:82 | Epoch 4: Val Loss = 0.4, F1-Macro = 0.58, Combined = 0.41, LR = 1.00e-03
2025-07-15 09:32:31 | INFO     | __main__:simulate_training_with_combined_metric:82 | Epoch 5: Val Loss = 0.4, F1-Macro = 0.58, Combined = 0.41, LR = 1.00e-03
2025-07-15 09:32:31 | INFO     | __main__:simulate_training_with_combined_metric:82 | Epoch 6: Val Loss = 0.4, F1-Macro = 0.58, Combined = 0.41, LR = 5.00e-04
2025-07-15 09:32:31 | INFO     | __main__:simulate_training_with_combined_metric:86 |    ðŸ”½ Learning rate reduced: 1.00e-03 â†’ 5.00e-04
2025-07-15 09:32:31 | INFO     | __main__:simulate_training_with_combined_metric:87 |    Reason: Combined metric plateaued (no improvement for 3 epochs)
2025-07-15 09:32:31 | INFO     | __main__:simulate_training_with_combined_metric:82 | Epoch 7: Val Loss = 0.4, F1-Macro = 0.58, Combined = 0.41, LR = 5.00e-04
2025-07-15 09:32:31 | INFO     | __main__:simulate_training_with_combined_metric:82 | Epoch 8: Val Loss = 0.3, F1-Macro = 1.00, Combined = 0.15, LR = 5.00e-04
2025-07-15 09:32:31 | INFO     | __main__:simulate_training_with_combined_metric:90 | 
2025-07-15 09:32:31 | INFO     | __main__:simulate_training_with_combined_metric:91 | ðŸ“Š LR Scheduler Summary:
2025-07-15 09:32:31 | INFO     | __main__:simulate_training_with_combined_metric:96 |    - Initial LR: 1.00e-03
2025-07-15 09:32:31 | INFO     | __main__:simulate_training_with_combined_metric:97 |    - Final LR: 5.00e-04
2025-07-15 09:32:31 | INFO     | __main__:simulate_training_with_combined_metric:98 |    - Total reductions: 1
2025-07-15 09:32:31 | INFO     | __main__:simulate_training_with_combined_metric:99 |    - LR reduction factor: 0.500
2025-07-15 09:32:31 | INFO     | __main__:simulate_training_with_combined_metric:101 | 
2025-07-15 09:32:31 | INFO     | __main__:simulate_training_with_combined_metric:102 | âœ… Combined metric plateau detection works correctly!
2025-07-15 09:32:31 | INFO     | __main__:simulate_training_with_combined_metric:103 | ðŸŽ¯ LR scheduler now uses same metric as model selection for consistency
2025-07-25 08:45:17 | INFO     | contrastive_trainer:load_justices_data:24 | Loading justices data from: data/raw/justices.json
2025-07-25 08:45:17 | INFO     | contrastive_trainer:load_justices_data:29 | Loaded 116 justices
2025-07-25 08:45:17 | INFO     | contrastive_trainer:split_justices:58 | Split justices: 93 train, 23 validation
2025-07-25 08:45:23 | INFO     | contrastive_trainer:train_model:134 | Model loaded with 36 truncated biography and 36 full biography tokenizations
2025-07-25 08:45:23 | INFO     | contrastive_trainer:prepare_dataset_dict:74 | Prepared 93 justice pairs for contrastive learning
2025-07-25 08:45:23 | INFO     | contrastive_trainer:prepare_dataset_dict:74 | Prepared 23 justice pairs for contrastive learning
2025-07-25 08:45:23 | INFO     | contrastive_trainer:train_model:176 | Created contrastive loss function with temperature 0.1 and alpha 0.5
2025-07-25 08:45:23 | INFO     | contrastive_trainer:train_model:187 | Starting training for 10 epochs
2025-07-25 08:45:33 | ERROR    | contrastive_trainer:train_model:211 | Error in batch 0: "Justice biography path not found in pre-tokenized truncated data: data/processed/bios/Philip P. Barbour.txt\nTried variations: ['data/processed/bios/Philip P. Barbour.txt', 'C:/Users/fabia/OneDrive/Documentos/GitHub/scotus_ai/data/processed/bios/Philip P. Barbour.txt', 'data/processed/bios/Philip P. Barbour.txt']\nAvailable paths (first 5): ['data/processed/bios/Abe_Fortas.txt', 'data/processed/bios/Amy_Coney_Barrett.txt', 'data/processed/bios/Anthony_Kennedy.txt', 'data/processed/bios/Antonin_Scalia.txt', 'data/processed/bios/Brett_Kavanaugh.txt']\nTotal available: 36"
2025-07-25 08:45:33 | ERROR    | contrastive_trainer:train_model:211 | Error in batch 1: "Justice biography path not found in pre-tokenized truncated data: data/processed/bios/Felix Frankfurter.txt\nTried variations: ['data/processed/bios/Felix Frankfurter.txt', 'C:/Users/fabia/OneDrive/Documentos/GitHub/scotus_ai/data/processed/bios/Felix Frankfurter.txt', 'data/processed/bios/Felix Frankfurter.txt']\nAvailable paths (first 5): ['data/processed/bios/Abe_Fortas.txt', 'data/processed/bios/Amy_Coney_Barrett.txt', 'data/processed/bios/Anthony_Kennedy.txt', 'data/processed/bios/Antonin_Scalia.txt', 'data/processed/bios/Brett_Kavanaugh.txt']\nTotal available: 36"
2025-07-25 08:45:33 | ERROR    | contrastive_trainer:train_model:211 | Error in batch 2: "Justice biography path not found in pre-tokenized truncated data: data/processed/bios/William J. Brennan Jr..txt\nTried variations: ['data/processed/bios/William J. Brennan Jr..txt', 'C:/Users/fabia/OneDrive/Documentos/GitHub/scotus_ai/data/processed/bios/William J. Brennan Jr..txt', 'data/processed/bios/William J. Brennan Jr..txt']\nAvailable paths (first 5): ['data/processed/bios/Abe_Fortas.txt', 'data/processed/bios/Amy_Coney_Barrett.txt', 'data/processed/bios/Anthony_Kennedy.txt', 'data/processed/bios/Antonin_Scalia.txt', 'data/processed/bios/Brett_Kavanaugh.txt']\nTotal available: 36"
2025-07-25 08:45:33 | ERROR    | contrastive_trainer:train_model:211 | Error in batch 3: "Justice biography path not found in pre-tokenized truncated data: data/processed/bios/Stephen Breyer.txt\nTried variations: ['data/processed/bios/Stephen Breyer.txt', 'C:/Users/fabia/OneDrive/Documentos/GitHub/scotus_ai/data/processed/bios/Stephen Breyer.txt', 'data/processed/bios/Stephen Breyer.txt']\nAvailable paths (first 5): ['data/processed/bios/Abe_Fortas.txt', 'data/processed/bios/Amy_Coney_Barrett.txt', 'data/processed/bios/Anthony_Kennedy.txt', 'data/processed/bios/Antonin_Scalia.txt', 'data/processed/bios/Brett_Kavanaugh.txt']\nTotal available: 36"
2025-07-25 08:45:33 | ERROR    | contrastive_trainer:train_model:211 | Error in batch 4: "Justice biography path not found in pre-tokenized truncated data: data/processed/bios/Sherman Minton.txt\nTried variations: ['data/processed/bios/Sherman Minton.txt', 'C:/Users/fabia/OneDrive/Documentos/GitHub/scotus_ai/data/processed/bios/Sherman Minton.txt', 'data/processed/bios/Sherman Minton.txt']\nAvailable paths (first 5): ['data/processed/bios/Abe_Fortas.txt', 'data/processed/bios/Amy_Coney_Barrett.txt', 'data/processed/bios/Anthony_Kennedy.txt', 'data/processed/bios/Antonin_Scalia.txt', 'data/processed/bios/Brett_Kavanaugh.txt']\nTotal available: 36"
2025-07-25 08:45:33 | ERROR    | contrastive_trainer:train_model:211 | Error in batch 5: "Justice biography path not found in pre-tokenized truncated data: data/processed/bios/Benjamin N. Cardozo.txt\nTried variations: ['data/processed/bios/Benjamin N. Cardozo.txt', 'C:/Users/fabia/OneDrive/Documentos/GitHub/scotus_ai/data/processed/bios/Benjamin N. Cardozo.txt', 'data/processed/bios/Benjamin N. Cardozo.txt']\nAvailable paths (first 5): ['data/processed/bios/Abe_Fortas.txt', 'data/processed/bios/Amy_Coney_Barrett.txt', 'data/processed/bios/Anthony_Kennedy.txt', 'data/processed/bios/Antonin_Scalia.txt', 'data/processed/bios/Brett_Kavanaugh.txt']\nTotal available: 36"
2025-07-25 08:45:33 | ERROR    | contrastive_trainer:train_model:211 | Error in batch 6: "Justice biography path not found in pre-tokenized truncated data: data/processed/bios/William Strong.txt\nTried variations: ['data/processed/bios/William Strong.txt', 'C:/Users/fabia/OneDrive/Documentos/GitHub/scotus_ai/data/processed/bios/William Strong.txt', 'data/processed/bios/William Strong.txt']\nAvailable paths (first 5): ['data/processed/bios/Abe_Fortas.txt', 'data/processed/bios/Amy_Coney_Barrett.txt', 'data/processed/bios/Anthony_Kennedy.txt', 'data/processed/bios/Antonin_Scalia.txt', 'data/processed/bios/Brett_Kavanaugh.txt']\nTotal available: 36"
2025-07-25 08:45:33 | ERROR    | contrastive_trainer:train_model:211 | Error in batch 7: "Justice biography path not found in pre-tokenized truncated data: data/processed/bios/Joseph Rucker Lamar.txt\nTried variations: ['data/processed/bios/Joseph Rucker Lamar.txt', 'C:/Users/fabia/OneDrive/Documentos/GitHub/scotus_ai/data/processed/bios/Joseph Rucker Lamar.txt', 'data/processed/bios/Joseph Rucker Lamar.txt']\nAvailable paths (first 5): ['data/processed/bios/Abe_Fortas.txt', 'data/processed/bios/Amy_Coney_Barrett.txt', 'data/processed/bios/Anthony_Kennedy.txt', 'data/processed/bios/Antonin_Scalia.txt', 'data/processed/bios/Brett_Kavanaugh.txt']\nTotal available: 36"
2025-07-25 08:45:33 | ERROR    | contrastive_trainer:train_model:211 | Error in batch 8: "Justice biography path not found in pre-tokenized truncated data: data/processed/bios/John Paul Stevens.txt\nTried variations: ['data/processed/bios/John Paul Stevens.txt', 'C:/Users/fabia/OneDrive/Documentos/GitHub/scotus_ai/data/processed/bios/John Paul Stevens.txt', 'data/processed/bios/John Paul Stevens.txt']\nAvailable paths (first 5): ['data/processed/bios/Abe_Fortas.txt', 'data/processed/bios/Amy_Coney_Barrett.txt', 'data/processed/bios/Anthony_Kennedy.txt', 'data/processed/bios/Antonin_Scalia.txt', 'data/processed/bios/Brett_Kavanaugh.txt']\nTotal available: 36"
2025-07-25 08:45:33 | ERROR    | contrastive_trainer:train_model:211 | Error in batch 9: "Justice biography path not found in pre-tokenized truncated data: data/processed/bios/Noah Haynes Swayne.txt\nTried variations: ['data/processed/bios/Noah Haynes Swayne.txt', 'C:/Users/fabia/OneDrive/Documentos/GitHub/scotus_ai/data/processed/bios/Noah Haynes Swayne.txt', 'data/processed/bios/Noah Haynes Swayne.txt']\nAvailable paths (first 5): ['data/processed/bios/Abe_Fortas.txt', 'data/processed/bios/Amy_Coney_Barrett.txt', 'data/processed/bios/Anthony_Kennedy.txt', 'data/processed/bios/Antonin_Scalia.txt', 'data/processed/bios/Brett_Kavanaugh.txt']\nTotal available: 36"
2025-07-25 08:45:33 | ERROR    | contrastive_trainer:train_model:211 | Error in batch 10: "Justice biography path not found in pre-tokenized truncated data: data/processed/bios/David J. Brewer.txt\nTried variations: ['data/processed/bios/David J. Brewer.txt', 'C:/Users/fabia/OneDrive/Documentos/GitHub/scotus_ai/data/processed/bios/David J. Brewer.txt', 'data/processed/bios/David J. Brewer.txt']\nAvailable paths (first 5): ['data/processed/bios/Abe_Fortas.txt', 'data/processed/bios/Amy_Coney_Barrett.txt', 'data/processed/bios/Anthony_Kennedy.txt', 'data/processed/bios/Antonin_Scalia.txt', 'data/processed/bios/Brett_Kavanaugh.txt']\nTotal available: 36"
2025-07-25 08:45:33 | ERROR    | contrastive_trainer:train_model:211 | Error in batch 11: "Justice biography path not found in pre-tokenized truncated data: data/processed/bios/Hugo Black.txt\nTried variations: ['data/processed/bios/Hugo Black.txt', 'C:/Users/fabia/OneDrive/Documentos/GitHub/scotus_ai/data/processed/bios/Hugo Black.txt', 'data/processed/bios/Hugo Black.txt']\nAvailable paths (first 5): ['data/processed/bios/Abe_Fortas.txt', 'data/processed/bios/Amy_Coney_Barrett.txt', 'data/processed/bios/Anthony_Kennedy.txt', 'data/processed/bios/Antonin_Scalia.txt', 'data/processed/bios/Brett_Kavanaugh.txt']\nTotal available: 36"
2025-07-25 08:45:34 | ERROR    | contrastive_trainer:train_model:215 | No successful training batches in this epoch
2025-07-25 08:45:34 | INFO     | contrastive_trainer:train_model:252 | Training completed successfully!
2025-07-25 09:59:21 | INFO     | contrastive_trainer:train_model:127 | Created contrastive loss function with temperature 0.1 and alpha 0.5
2025-07-25 09:59:21 | INFO     | contrastive_trainer:train_model:138 | Starting training for 10 epochs
2025-07-25 09:59:35 | ERROR    | contrastive_trainer:train_model:162 | Error in batch 0: Torch not compiled with CUDA enabled
2025-07-25 09:59:36 | ERROR    | contrastive_trainer:train_model:166 | No successful training batches in this epoch
2025-07-25 09:59:36 | INFO     | contrastive_trainer:train_model:203 | Training completed successfully!
