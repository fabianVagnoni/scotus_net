# SCOTUS AI Model Configuration
# =============================
# Centralized hyperparameters for model training and architecture

# Model Architecture
# ------------------
# Sentence transformer model for justice biographies
BIO_MODEL_NAME=sentence-transformers/all-MiniLM-L6-v2
# Sentence transformer model for case descriptions (legal domain)
DESCRIPTION_MODEL_NAME=Stern5497/sbert-legal-xlm-roberta-base

# Embedding dimensions (should match sentence transformer output)
EMBEDDING_DIM=384
# Hidden layer dimension for fully connected layers
HIDDEN_DIM=512
# Maximum number of justices that can be on the court
MAX_JUSTICES=15

# Attention Mechanism
# ------------------
# Whether to use justice cross-attention or simple concatenation
USE_JUSTICE_ATTENTION=true
# Number of attention heads for justice cross-attention
NUM_ATTENTION_HEADS=16

# Regularization
# --------------
# Dropout rate for regularization
DROPOUT_RATE=0.1
# Weight decay for optimizer
WEIGHT_DECAY=0.01

# Training Configuration
# ---------------------
# Learning rate for optimizer
LEARNING_RATE=0.0001
# Number of training epochs
NUM_EPOCHS=10
# Batch size for training (keep small due to memory constraints)
BATCH_SIZE=16
# Number of data loader workers (0 to avoid multiprocessing issues)
NUM_WORKERS=0

# Early Stopping and Scheduling
# -----------------------------
# Patience for early stopping (epochs without improvement)
PATIENCE=5
# Learning rate scheduler factor (multiply LR by this when plateau)
LR_SCHEDULER_FACTOR=0.5
# Learning rate scheduler patience (epochs to wait before reducing LR)
LR_SCHEDULER_PATIENCE=3

# Gradient Clipping
# ----------------
# Maximum gradient norm for clipping
MAX_GRAD_NORM=1.0

# Device Configuration
# -------------------
# Options: cuda, cpu, auto (auto will detect available device)
DEVICE=auto

# Data Paths
# ----------
# Path to case dataset JSON file
DATASET_FILE=data/processed/case_dataset.json
# Path to pre-tokenized biography data
BIO_TOKENIZED_FILE=data/processed/encoded_bios.pkl
# Path to pre-tokenized case description data
DESCRIPTION_TOKENIZED_FILE=data/processed/encoded_descriptions.pkl

# Output Paths
# -----------
# Directory to save trained models
MODEL_OUTPUT_DIR=models_output
# Best model filename
BEST_MODEL_NAME=best_model.pth

# Dataset Splitting
# ----------------
# Training set ratio
TRAIN_RATIO=0.7
# Validation set ratio
VAL_RATIO=0.15
# Test set ratio
TEST_RATIO=0.15
# Random seed for dataset splitting
SPLIT_RANDOM_STATE=42

# Loss Function
# ------------
# Loss function type: kl_div, mse, focal, weighted_focal
LOSS_FUNCTION=kl_div
# KL divergence reduction method: batchmean, sum, mean
KL_REDUCTION=batchmean

# Focal Loss Parameters (for handling class imbalance)
# ---------------------------------------------------
# Initial gamma value for focal loss (higher = more focus on hard examples)
FOCAL_GAMMA_INITIAL=2.0
# Final gamma value for focal loss annealing
FOCAL_GAMMA_FINAL=1.0
# Exponential decay rate for gamma annealing
FOCAL_GAMMA_DECAY_RATE=0.1
# Power to raise inverse frequency weights for weighted focal loss
FOCAL_WEIGHT_POWER=1.0

# Validation and Evaluation
# -------------------------
# How often to validate during training (every N batches)
VALIDATION_FREQUENCY=10
# Whether to evaluate on test set after training
EVALUATE_ON_TEST=true

# Logging and Progress
# -------------------
# Whether to show detailed progress during training
VERBOSE_TRAINING=true
# How often to log training progress (every N batches)
LOG_FREQUENCY=10

# Memory Management
# ----------------
# Whether to clear GPU cache on memory errors
CLEAR_CACHE_ON_OOM=true

# Model Saving
# -----------
# Whether to save model checkpoints during training
SAVE_CHECKPOINTS=true
# How often to save checkpoints (every N epochs)
CHECKPOINT_FREQUENCY=5

# Cross-Attention Specific
# -----------------------
# Whether to use feed-forward network in attention
USE_ATTENTION_FFN=true
# Multiplier for FFN hidden dimension (relative to embedding_dim)
ATTENTION_FFN_MULTIPLIER=2

# Advanced Training
# ----------------
# Whether to use mixed precision training (if supported)
USE_MIXED_PRECISION=false
# Gradient accumulation steps (to simulate larger batch sizes)
GRADIENT_ACCUMULATION_STEPS=1

# Sentence Transformer Fine-tuning
# --------------------------------
# Whether to enable fine-tuning of sentence transformers
ENABLE_SENTENCE_TRANSFORMER_FINETUNING=true
# Epoch at which to unfreeze sentence transformers (-1 to never unfreeze, 0 to start unfrozen)
UNFREEZE_SENTENCE_TRANSFORMERS_EPOCH=3
# Learning rate for sentence transformer fine-tuning (usually lower than main LR)
SENTENCE_TRANSFORMER_LEARNING_RATE=1e-5
# Whether to unfreeze bio model (justice biographies)
UNFREEZE_BIO_MODEL=true
# Whether to unfreeze description model (case descriptions)
UNFREEZE_DESCRIPTION_MODEL=true

# Progressive Unfreezing Strategy
# ------------------------------
# Whether to use progressive unfreezing (first N layers, then all layers)
USE_PROGRESSIVE_UNFREEZING=true
# Number of final layers to unfreeze in the first step (0 = unfreeze all layers at once)
INITIAL_LAYERS_TO_UNFREEZE=3
# Epoch at which to perform the second unfreezing step (unfreeze all layers)
SECOND_UNFREEZE_EPOCH=5
# Whether to only do partial unfreezing (never unfreeze all layers)
ONLY_PARTIAL_UNFREEZING=false

# Learning Rate Reduction Strategy
# --------------------------------
# Factor to reduce learning rate after first unfreezing step
LR_REDUCTION_FACTOR_FIRST_UNFREEZE=0.5
# Factor to reduce learning rate after second unfreezing step
LR_REDUCTION_FACTOR_SECOND_UNFREEZE=0.3
# Whether to apply LR reduction to the main optimizer as well
REDUCE_MAIN_LR_ON_UNFREEZE=true

# Random Seeds
# -----------
# Random seed for reproducibility
RANDOM_SEED=42
# PyTorch random seed
TORCH_SEED=42
# NumPy random seed
NUMPY_SEED=42

# Model Loading and Caching
# -------------------------
# Whether to use local cache for sentence transformer models
USE_MODEL_CACHE=true
# Whether to download models if not found locally
DOWNLOAD_MODELS=true

# Data Validation
# --------------
# Minimum number of justices required per case
MIN_JUSTICES_PER_CASE=1
# Maximum number of justices allowed per case
MAX_JUSTICES_PER_CASE=15
# Whether to skip cases with missing descriptions
SKIP_MISSING_DESCRIPTIONS=true
# Whether to skip cases with no valid justice biographies
SKIP_MISSING_BIOGRAPHIES=true

# =============================================================================
# HYPERPARAMETER OPTIMIZATION CONFIGURATION
# =============================================================================
# This section configures hyperparameter optimization with a simplified approach:
# - 10 tuned parameters (reduced from 12) for more efficient optimization
# - Fixed values for parameters with clear best practices
# - Maintains essential progressive unfreezing capabilities
# 
# Current tuned parameters: hidden_dim, dropout_rate, num_attention_heads, 
# learning_rate, batch_size, weight_decay, use_progressive_unfreezing, 
# initial_layers_to_unfreeze, lr_reduction_factor_first, lr_reduction_factor_second
# =============================================================================

# Study Configuration

# Hyperparameter Optimization
# ---------------------------
# Number of optimization trials to run
OPTUNA_N_TRIALS=100
# Maximum time per trial in seconds (0 for no limit)
OPTUNA_MAX_TRIAL_TIME=1200
# Maximum epochs per trial during optimization
OPTUNA_MAX_EPOCHS=7
# Minimum epochs before early stopping can occur
OPTUNA_MIN_EPOCHS=2
# Early stopping patience for optimization trials
OPTUNA_EARLY_STOP_PATIENCE=2
# Maximum training samples per trial (0 for no limit)
OPTUNA_MAX_TRAIN_SAMPLES=2100
# Maximum validation samples per trial (0 for no limit)
OPTUNA_MAX_VAL_SAMPLES=700
# Optuna pruner startup trials
OPTUNA_PRUNER_STARTUP_TRIALS=10
# Optuna pruner warmup steps
OPTUNA_PRUNER_WARMUP_STEPS=3

# Hyperparameter Tuning Control
# -----------------------------
# Controls which hyperparameters to optimize during hyperparameter tuning.
# Set to 'true' to allow Optuna to suggest values for the parameter.
# Set to 'false' to use the default value defined above in this config file.
#
# This allows you to selectively tune only certain parameters while keeping
# others fixed to their default values, which can be useful for:
# - Focusing optimization on specific aspects of the model
# - Reducing optimization time by tuning fewer parameters
# - Comparing the effect of specific hyperparameters
#
# Example: To only tune learning rate and dropout while keeping architecture fixed:
#   TUNE_LEARNING_RATE=true
#   TUNE_DROPOUT_RATE=true  
#   TUNE_HIDDEN_DIM=false
#   TUNE_NUM_ATTENTION_HEADS=false
#   etc.

# Model Architecture Control
TUNE_HIDDEN_DIM=true                    # Tune hidden dimension
TUNE_DROPOUT_RATE=true                  # Tune dropout rate
TUNE_NUM_ATTENTION_HEADS=true           # Tune number of attention heads
TUNE_USE_JUSTICE_ATTENTION=false        # FIX: Use justice attention (likely beneficial)

# Training Parameters Control
TUNE_LEARNING_RATE=true                 # Tune learning rate
TUNE_BATCH_SIZE=true                    # Tune batch size
TUNE_WEIGHT_DECAY=true                  # Tune weight decay

# Sentence Transformer Fine-tuning Control
TUNE_UNFREEZE_EPOCH=false               # FIX: Use reasonable default unfreezing epoch

# Progressive Unfreezing Control
TUNE_USE_PROGRESSIVE_UNFREEZING=true    # Tune whether to use progressive unfreezing
TUNE_INITIAL_LAYERS_TO_UNFREEZE=true    # Tune number of layers to initially unfreeze
TUNE_SECOND_UNFREEZE_EPOCH=false         # Tune when to do second unfreezing step
TUNE_ONLY_PARTIAL_UNFREEZING=false       # Tune whether to only do partial unfreezing

# Learning Rate Reduction Control
TUNE_LR_REDUCTION_FACTOR_FIRST=true     # Tune LR reduction factor after first unfreeze
TUNE_LR_REDUCTION_FACTOR_SECOND=true    # Tune LR reduction factor after second unfreeze

# Hyperparameter Search Spaces
# ----------------------------
# Hidden dimension options (comma-separated)
OPTUNA_HIDDEN_DIM_OPTIONS=256,512,1024
# Dropout rate range (min,max,step)
OPTUNA_DROPOUT_RATE_RANGE=0.05,0.5,0.15
# Number of attention heads options (comma-separated)
OPTUNA_ATTENTION_HEADS_OPTIONS=4,8,32
# Justice attention options (comma-separated boolean)
OPTUNA_JUSTICE_ATTENTION_OPTIONS=true,false
# Learning rate range (min,max,log_scale)
OPTUNA_LEARNING_RATE_RANGE=1e-5,1e-3,true
# Batch size options (comma-separated)
OPTUNA_BATCH_SIZE_OPTIONS=8,16
# Weight decay range (min,max,log_scale)
OPTUNA_WEIGHT_DECAY_RANGE=1e-4,1e-1,true

# Progressive Unfreezing Search Spaces
# ------------------------------------
# Whether to use progressive unfreezing (comma-separated boolean)
OPTUNA_USE_PROGRESSIVE_UNFREEZING_OPTIONS=true,false
# Number of layers to initially unfreeze (comma-separated integers, 0 means unfreeze all)
OPTUNA_INITIAL_LAYERS_TO_UNFREEZE_OPTIONS=0,2,4
# Second unfreeze epoch options (comma-separated integers, -1 means no second unfreeze)
OPTUNA_SECOND_UNFREEZE_EPOCH_OPTIONS=-1,4,6
# Whether to only do partial unfreezing (comma-separated boolean)
OPTUNA_ONLY_PARTIAL_UNFREEZING_OPTIONS=true,false

# Progressive Unfreezing Strategy Options:
# - use_progressive_unfreezing=false: Traditional unfreezing (unfreeze all at once)
# - use_progressive_unfreezing=true, initial_layers=0: Unfreeze all at once via progressive framework
# - use_progressive_unfreezing=true, initial_layers=N, second_epoch=-1: Only unfreeze N layers (partial unfreezing)
# - use_progressive_unfreezing=true, initial_layers=N, second_epoch=M: Two-step progressive unfreezing

# Learning Rate Reduction Search Spaces
# -------------------------------------
# LR reduction factor after first unfreeze (min,max,step)
OPTUNA_LR_REDUCTION_FACTOR_FIRST_RANGE=0.05,0.35,0.15
# LR reduction factor after second unfreeze (min,max,step)
OPTUNA_LR_REDUCTION_FACTOR_SECOND_RANGE=0.05,0.35,0.15