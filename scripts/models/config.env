# SCOTUS AI Model Configuration
# =============================
# Centralized hyperparameters for model training and architecture

# Model Architecture
# ------------------
# Sentence transformer model for justice biographies
BIO_MODEL_NAME=sentence-transformers/all-MiniLM-L6-v2
# Sentence transformer model for case descriptions (legal domain)
DESCRIPTION_MODEL_NAME=Stern5497/sbert-legal-xlm-roberta-base

# Embedding dimensions (should match sentence transformer output)
EMBEDDING_DIM=384
# Hidden layer dimension for fully connected layers
HIDDEN_DIM=512
# Maximum number of justices that can be on the court
MAX_JUSTICES=15

# Attention Mechanism
# ------------------
# Whether to use justice cross-attention or simple concatenation
USE_JUSTICE_ATTENTION=true
# Number of attention heads for justice cross-attention
NUM_ATTENTION_HEADS=16

# Regularization
# --------------
# Dropout rate for regularization
DROPOUT_RATE=0.1
# Weight decay for optimizer
WEIGHT_DECAY=0.01

# Training Configuration
# ---------------------
# Learning rate for optimizer
LEARNING_RATE=0.0001
# Number of training epochs
NUM_EPOCHS=10
# Batch size for training (keep small due to memory constraints)
BATCH_SIZE=16
# Number of data loader workers (0 to avoid multiprocessing issues)
NUM_WORKERS=0

# Early Stopping and Scheduling
# -----------------------------
# Patience for early stopping (epochs without improvement)
PATIENCE=5
# Learning rate scheduler factor (multiply LR by this when plateau)
LR_SCHEDULER_FACTOR=0.5
# Learning rate scheduler patience (epochs to wait before reducing LR)
LR_SCHEDULER_PATIENCE=3

# Gradient Clipping
# ----------------
# Maximum gradient norm for clipping
MAX_GRAD_NORM=1.0

# Device Configuration
# -------------------
# Options: cuda, cpu, auto (auto will detect available device)
DEVICE=auto

# Data Paths
# ----------
# Path to case dataset JSON file
DATASET_FILE=data/processed/case_dataset.json
# Path to pre-tokenized biography data
BIO_TOKENIZED_FILE=data/processed/encoded_bios.pkl
# Path to pre-tokenized case description data
DESCRIPTION_TOKENIZED_FILE=data/processed/encoded_descriptions.pkl

# Output Paths
# -----------
# Directory to save trained models
MODEL_OUTPUT_DIR=models_output
# Best model filename
BEST_MODEL_NAME=best_model.pth

# Dataset Splitting
# ----------------
# Training set ratio
TRAIN_RATIO=0.7
# Validation set ratio
VAL_RATIO=0.15
# Test set ratio
TEST_RATIO=0.15
# Random seed for dataset splitting
SPLIT_RANDOM_STATE=42

# Loss Function
# ------------
# Loss function type: kl_div, mse, cross_entropy
LOSS_FUNCTION=kl_div
# KL divergence reduction method: batchmean, sum, mean
KL_REDUCTION=batchmean

# Validation and Evaluation
# -------------------------
# How often to validate during training (every N batches)
VALIDATION_FREQUENCY=10
# Whether to evaluate on test set after training
EVALUATE_ON_TEST=true

# Logging and Progress
# -------------------
# Whether to show detailed progress during training
VERBOSE_TRAINING=true
# How often to log training progress (every N batches)
LOG_FREQUENCY=10

# Memory Management
# ----------------
# Whether to clear GPU cache on memory errors
CLEAR_CACHE_ON_OOM=true

# Model Saving
# -----------
# Whether to save model checkpoints during training
SAVE_CHECKPOINTS=true
# How often to save checkpoints (every N epochs)
CHECKPOINT_FREQUENCY=5

# Cross-Attention Specific
# -----------------------
# Whether to use feed-forward network in attention
USE_ATTENTION_FFN=true
# Multiplier for FFN hidden dimension (relative to embedding_dim)
ATTENTION_FFN_MULTIPLIER=2

# Advanced Training
# ----------------
# Whether to use mixed precision training (if supported)
USE_MIXED_PRECISION=false
# Gradient accumulation steps (to simulate larger batch sizes)
GRADIENT_ACCUMULATION_STEPS=1

# Random Seeds
# -----------
# Random seed for reproducibility
RANDOM_SEED=42
# PyTorch random seed
TORCH_SEED=42
# NumPy random seed
NUMPY_SEED=42

# Model Loading and Caching
# -------------------------
# Whether to use local cache for sentence transformer models
USE_MODEL_CACHE=true
# Whether to download models if not found locally
DOWNLOAD_MODELS=true

# Data Validation
# --------------
# Minimum number of justices required per case
MIN_JUSTICES_PER_CASE=1
# Maximum number of justices allowed per case
MAX_JUSTICES_PER_CASE=15
# Whether to skip cases with missing descriptions
SKIP_MISSING_DESCRIPTIONS=true
# Whether to skip cases with no valid justice biographies
SKIP_MISSING_BIOGRAPHIES=true 