# SCOTUS AI Model Configuration
# =============================
# Centralized hyperparameters for model training and architecture

# Model Architecture
# ------------------
# Sentence transformer model for justice biographies
BIO_MODEL_NAME=sentence-transformers/all-roberta-large-v1
# Sentence transformer model for case descriptions (legal domain)
DESCRIPTION_MODEL_NAME=sentence-transformers/all-roberta-large-v1

# Embedding dimensions (should match sentence transformer output)
EMBEDDING_DIM=384
# Hidden layer dimension for fully connected layers
HIDDEN_DIM=2048
# Maximum number of justices that can be on the court
MAX_JUSTICES=11

# Attention Mechanism
# ------------------
# Whether to use justice cross-attention or simple concatenation
USE_JUSTICE_ATTENTION=false
# Number of attention heads for justice cross-attention
NUM_ATTENTION_HEADS=16

# Regularization
# --------------
# Dropout rate for regularization
DROPOUT_RATE=0.25
# Weight decay for optimizer
WEIGHT_DECAY=3.578246305582672e-05
# Use NEFTune regularization
USE_NOISE_REG=true
# NEFTune's Alpha scaler
NOISE_REG_ALPHA=5

# Training Configuration
# ---------------------
# Learning rate for optimizer
LEARNING_RATE=0.0002028596158885991
# Number of training epochs
NUM_EPOCHS=10
# Batch size for training (keep small due to memory constraints)
BATCH_SIZE=16
# Number of data loader workers (0 to avoid multiprocessing issues)
NUM_WORKERS=4

# Early Stopping and Scheduling
# -----------------------------
# Patience for early stopping (epochs without improvement)
PATIENCE=10
# Learning rate scheduler factor (multiply LR by this when plateau)
LR_SCHEDULER_FACTOR=0.5
# Learning rate scheduler patience (epochs to wait before reducing LR)
LR_SCHEDULER_PATIENCE=3

# Gradient Clipping
# ----------------
# Maximum gradient norm for clipping
MAX_GRAD_NORM=1.0

# Device Configuration
# -------------------
# Options: cuda, cpu, auto (auto will detect available device)
DEVICE=auto

# Data Paths
# ----------
# Path to case dataset JSON file
DATASET_FILE=data/processed/case_dataset.json
# Path to pre-tokenized biography data
BIO_TOKENIZED_FILE=data/processed/encoded_bios.pkl
# Path to pre-tokenized case description data
DESCRIPTION_TOKENIZED_FILE=data/processed/encoded_descriptions.pkl

# Output Paths
# -----------
# Directory to save trained models
MODEL_OUTPUT_DIR=logs/training_logs
# Best model filename
BEST_MODEL_NAME=best_model.pth

# Dataset Splitting
# ----------------
# Training set ratio
TRAIN_RATIO=0.7
# Validation set ratio
VAL_RATIO=0.15
# Test set ratio
TEST_RATIO=0.15
# Random seed for dataset splitting
SPLIT_RANDOM_STATE=42

# Loss Function
# ------------
# Loss function type: kl_div, mse, focal, weighted_focal
LOSS_FUNCTION=kl_div
# KL divergence reduction method: batchmean, sum, mean
KL_REDUCTION=batchmean

# Focal Loss Parameters (for handling class imbalance)
# ---------------------------------------------------
# Initial gamma value for focal loss (higher = more focus on hard examples)
FOCAL_GAMMA_INITIAL=2.0
# Final gamma value for focal loss annealing
FOCAL_GAMMA_FINAL=1.0
# Exponential decay rate for gamma annealing
FOCAL_GAMMA_DECAY_RATE=0.1
# Power to raise inverse frequency weights for weighted focal loss
FOCAL_WEIGHT_POWER=1.0

# Validation and Evaluation
# -------------------------
# How often to validate during training (every N batches)
VALIDATION_FREQUENCY=10
# Whether to evaluate on test set after training
EVALUATE_ON_TEST=true

# Logging and Progress
# -------------------
# Whether to show detailed progress during training
VERBOSE_TRAINING=true
# How often to log training progress (every N batches)
LOG_FREQUENCY=10

# Memory Management
# ----------------
# Whether to clear GPU cache on memory errors
CLEAR_CACHE_ON_OOM=true

# Model Saving
# -----------
# Whether to save model checkpoints during training
SAVE_CHECKPOINTS=true
# How often to save checkpoints (every N epochs)
CHECKPOINT_FREQUENCY=5

# Cross-Attention Specific
# -----------------------
# Whether to use feed-forward network in attention
USE_ATTENTION_FFN=true
# Multiplier for FFN hidden dimension (relative to embedding_dim)
ATTENTION_FFN_MULTIPLIER=2

# Advanced Training
# ----------------
# Whether to use mixed precision training (if supported)
USE_MIXED_PRECISION=false
# Gradient accumulation steps (to simulate larger batch sizes)
GRADIENT_ACCUMULATION_STEPS=1

# Sentence Transformer Fine-tuning Strategy
# ----------------------------------------
# Whether to enable fine-tuning of sentence transformers
ENABLE_SENTENCE_TRANSFORMER_FINETUNING=true
# Learning rate for sentence transformer fine-tuning (usually lower than main LR)
SENTENCE_TRANSFORMER_LEARNING_RATE=1e-5
# Whether to unfreeze bio model (justice biographies)
UNFREEZE_BIO_MODEL=true
# Whether to unfreeze description model (case descriptions)
UNFREEZE_DESCRIPTION_MODEL=true

# Three-Step Fine-tuning Strategy
# -------------------------------
# Step 1: Train only the head (frozen underlying models)
# Step 2: Unfreeze N last layers + reduce LR
# Step 3: Unfreeze all layers + reduce LR further
#
# Epoch to start Step 2 (partial unfreezing, -1 to skip Step 2)
FIRST_UNFREEZE_EPOCH=2
# Epoch to start Step 3 (full unfreezing, -1 to skip Step 3)
SECOND_UNFREEZE_EPOCH=4
# Number of final layers to unfreeze in Step 2 (ignored if FIRST_UNFREEZE_EPOCH=-1)
INITIAL_LAYERS_TO_UNFREEZE=4
# Learning rate reduction factor (applied cumulatively at each step)
LR_REDUCTION_FACTOR=0.3
# Whether to apply LR reduction to the main optimizer as well
REDUCE_MAIN_LR_ON_UNFREEZE=true

# Random Seeds
# -----------
# Random seed for reproducibility
RANDOM_SEED=42
# PyTorch random seed
TORCH_SEED=42
# NumPy random seed
NUMPY_SEED=42

# Model Loading and Caching
# -------------------------
# Whether to use local cache for sentence transformer models
USE_MODEL_CACHE=true
# Whether to download models if not found locally
DOWNLOAD_MODELS=true

# Data Validation
# --------------
# Minimum number of justices required per case
MIN_JUSTICES_PER_CASE=1
# Maximum number of justices allowed per case
MAX_JUSTICES_PER_CASE=9
# Whether to skip cases with missing descriptions
SKIP_MISSING_DESCRIPTIONS=true
# Whether to skip cases with no valid justice biographies
SKIP_MISSING_BIOGRAPHIES=true

# =============================================================================
# HYPERPARAMETER OPTIMIZATION CONFIGURATION
# =============================================================================
# This section configures hyperparameter optimization with a simplified approach:
# - 8 core parameters for efficient optimization
# - Simplified fine-tuning strategy with clear control
# - Single control flag for fine-tuning strategy tuning
# 
# Current tuned parameters: hidden_dim, dropout_rate, num_attention_heads, 
# learning_rate, batch_size, weight_decay, and fine-tuning strategy 
# (first_unfreeze_epoch, second_unfreeze_epoch, initial_layers_to_unfreeze, lr_reduction_factor)
# =============================================================================

# Study Configuration

# Hyperparameter Optimization
# ---------------------------
# Number of optimization trials to run
OPTUNA_N_TRIALS=70
# Maximum time per trial in seconds (0 for no limit)
OPTUNA_MAX_TRIAL_TIME=1500
# Maximum epochs per trial during optimization
OPTUNA_MAX_EPOCHS=8
# Minimum epochs before early stopping can occur
OPTUNA_MIN_EPOCHS=3
# Early stopping patience for optimization trials
OPTUNA_EARLY_STOP_PATIENCE=2
# Maximum training samples per trial (0 for no limit)
OPTUNA_MAX_TRAIN_SAMPLES=1500
# Maximum validation samples per trial (0 for no limit)
OPTUNA_MAX_VAL_SAMPLES=200
# Optuna pruner startup trials
OPTUNA_PRUNER_STARTUP_TRIALS=10
# Optuna pruner warmup steps
OPTUNA_PRUNER_WARMUP_STEPS=3

# Hyperparameter Tuning Control
# -----------------------------
# Controls which hyperparameters to optimize during hyperparameter tuning.
# Set to 'true' to allow Optuna to suggest values for the parameter.
# Set to 'false' to use the default value defined above in this config file.
#
# This allows you to selectively tune only certain parameters while keeping
# others fixed to their default values, which can be useful for:
# - Focusing optimization on specific aspects of the model
# - Reducing optimization time by tuning fewer parameters
# - Comparing the effect of specific hyperparameters
#
# Example: To only tune learning rate and dropout while keeping architecture fixed:
#   TUNE_LEARNING_RATE=true
#   TUNE_DROPOUT_RATE=true  
#   TUNE_HIDDEN_DIM=false
#   TUNE_NUM_ATTENTION_HEADS=false
#   etc.

# Model Architecture Control
TUNE_HIDDEN_DIM=true                    # Tune hidden dimension
TUNE_DROPOUT_RATE=false                  # Tune dropout rate
TUNE_NUM_ATTENTION_HEADS=false           # Tune number of attention heads
TUNE_USE_JUSTICE_ATTENTION=false         # FIX: Use justice attention (likely beneficial)

# Training Parameters Control
TUNE_LEARNING_RATE=true                 # Tune learning rate
TUNE_BATCH_SIZE=false                   # Tune batch size
TUNE_WEIGHT_DECAY=true                  # Tune weight decay
TUNE_NOISE_REG=true                     # Tune both use_noise_reg and noise_reg_alpha together

# Fine-tuning Strategy Control
TUNE_FINE_TUNING_STRATEGY=true          # Enable/disable tuning of the fine-tuning strategy

# Hyperparameter Search Spaces
# ----------------------------
# Hidden dimension options (comma-separated)
OPTUNA_HIDDEN_DIM_OPTIONS=768,1024,2048
# Dropout rate range (min,max,step)
OPTUNA_DROPOUT_RATE_RANGE=0.05,0.2,0.05
# Number of attention heads options (comma-separated)
OPTUNA_ATTENTION_HEADS_OPTIONS=8,12,16
# Justice attention options (comma-separated boolean)
OPTUNA_JUSTICE_ATTENTION_OPTIONS=true,false

# Learning rate range (min,max,log_scale)
OPTUNA_LEARNING_RATE_RANGE=1e-4,8e-4,true
# Batch size options (comma-separated)
OPTUNA_BATCH_SIZE_OPTIONS=8,16
# Weight decay range (min,max,log_scale)
OPTUNA_WEIGHT_DECAY_RANGE=1e-5,5e-3,true
# Use of NEFTune (when TUNE_NOISE_REG=true, both are tuned together)
OPTUNA_USE_NOISE_REG=true,false
# NEFTune's Alpha (only tuned when use_noise_reg=true)
OPTUNA_NOISE_ALPHA=0.1,10,false

# Fine-tuning Strategy Search Spaces
# ----------------------------------
# First unfreeze epoch options (comma-separated integers, -1 means skip Step 2)
OPTUNA_FIRST_UNFREEZE_EPOCH_OPTIONS=2,4
# Second unfreeze epoch options (comma-separated integers, -1 means skip Step 3)
OPTUNA_SECOND_UNFREEZE_EPOCH_OPTIONS=-1,5
# Number of layers to initially unfreeze in Step 2 (comma-separated integers)
OPTUNA_INITIAL_LAYERS_TO_UNFREEZE_OPTIONS=3,4,6
# Learning rate reduction factor (min,max,step) - applied cumulatively
OPTUNA_LR_REDUCTION_FACTOR_RANGE=0.3,0.5,0.1

# Fine-tuning Strategy Options:
# - FIRST_UNFREEZE_EPOCH=-1, SECOND_UNFREEZE_EPOCH=-1: No fine-tuning (frozen models)
# - FIRST_UNFREEZE_EPOCH=-1, SECOND_UNFREEZE_EPOCH=N: Skip Step 2, go directly to Step 3
# - FIRST_UNFREEZE_EPOCH=M, SECOND_UNFREEZE_EPOCH=-1: Only Step 2 (partial unfreezing)
# - FIRST_UNFREEZE_EPOCH=M, SECOND_UNFREEZE_EPOCH=N: Full three-step strategy