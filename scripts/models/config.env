# SCOTUS AI Model Configuration
# =============================
# Centralized hyperparameters for model training and architecture

# Model Architecture
# ------------------
# Sentence transformer model for justice biographies
BIO_MODEL_NAME=sentence-transformers/all-MiniLM-L6-v2
# Sentence transformer model for case descriptions (legal domain)
DESCRIPTION_MODEL_NAME=Stern5497/sbert-legal-xlm-roberta-base

# Embedding dimensions (should match sentence transformer output)
EMBEDDING_DIM=384
# Hidden layer dimension for fully connected layers
HIDDEN_DIM=512
# Maximum number of justices that can be on the court
MAX_JUSTICES=15

# Attention Mechanism
# ------------------
# Whether to use justice cross-attention or simple concatenation
USE_JUSTICE_ATTENTION=true
# Number of attention heads for justice cross-attention
NUM_ATTENTION_HEADS=16

# Regularization
# --------------
# Dropout rate for regularization
DROPOUT_RATE=0.1
# Weight decay for optimizer
WEIGHT_DECAY=0.01

# Training Configuration
# ---------------------
# Learning rate for optimizer
LEARNING_RATE=0.0001
# Number of training epochs
NUM_EPOCHS=10
# Batch size for training (keep small due to memory constraints)
BATCH_SIZE=16
# Number of data loader workers (0 to avoid multiprocessing issues)
NUM_WORKERS=0

# Early Stopping and Scheduling
# -----------------------------
# Patience for early stopping (epochs without improvement)
PATIENCE=5
# Learning rate scheduler factor (multiply LR by this when plateau)
LR_SCHEDULER_FACTOR=0.5
# Learning rate scheduler patience (epochs to wait before reducing LR)
LR_SCHEDULER_PATIENCE=3

# Gradient Clipping
# ----------------
# Maximum gradient norm for clipping
MAX_GRAD_NORM=1.0

# Device Configuration
# -------------------
# Options: cuda, cpu, auto (auto will detect available device)
DEVICE=auto

# Data Paths
# ----------
# Path to case dataset JSON file
DATASET_FILE=data/processed/case_dataset.json
# Path to pre-tokenized biography data
BIO_TOKENIZED_FILE=data/processed/encoded_bios.pkl
# Path to pre-tokenized case description data
DESCRIPTION_TOKENIZED_FILE=data/processed/encoded_descriptions.pkl

# Output Paths
# -----------
# Directory to save trained models
MODEL_OUTPUT_DIR=models_output
# Best model filename
BEST_MODEL_NAME=best_model.pth

# Dataset Splitting
# ----------------
# Training set ratio
TRAIN_RATIO=0.7
# Validation set ratio
VAL_RATIO=0.15
# Test set ratio
TEST_RATIO=0.15
# Random seed for dataset splitting
SPLIT_RANDOM_STATE=42

# Loss Function
# ------------
# Loss function type: kl_div, mse, cross_entropy
LOSS_FUNCTION=kl_div
# KL divergence reduction method: batchmean, sum, mean
KL_REDUCTION=batchmean

# Validation and Evaluation
# -------------------------
# How often to validate during training (every N batches)
VALIDATION_FREQUENCY=10
# Whether to evaluate on test set after training
EVALUATE_ON_TEST=true

# Logging and Progress
# -------------------
# Whether to show detailed progress during training
VERBOSE_TRAINING=true
# How often to log training progress (every N batches)
LOG_FREQUENCY=10

# Memory Management
# ----------------
# Whether to clear GPU cache on memory errors
CLEAR_CACHE_ON_OOM=true

# Model Saving
# -----------
# Whether to save model checkpoints during training
SAVE_CHECKPOINTS=true
# How often to save checkpoints (every N epochs)
CHECKPOINT_FREQUENCY=5

# Cross-Attention Specific
# -----------------------
# Whether to use feed-forward network in attention
USE_ATTENTION_FFN=true
# Multiplier for FFN hidden dimension (relative to embedding_dim)
ATTENTION_FFN_MULTIPLIER=2

# Advanced Training
# ----------------
# Whether to use mixed precision training (if supported)
USE_MIXED_PRECISION=false
# Gradient accumulation steps (to simulate larger batch sizes)
GRADIENT_ACCUMULATION_STEPS=1

# Sentence Transformer Fine-tuning
# --------------------------------
# Whether to enable fine-tuning of sentence transformers
ENABLE_SENTENCE_TRANSFORMER_FINETUNING=true
# Epoch at which to unfreeze sentence transformers (-1 to never unfreeze, 0 to start unfrozen)
UNFREEZE_SENTENCE_TRANSFORMERS_EPOCH=2
# Learning rate for sentence transformer fine-tuning (usually lower than main LR)
SENTENCE_TRANSFORMER_LEARNING_RATE=1e-5
# Whether to unfreeze bio model (justice biographies)
UNFREEZE_BIO_MODEL=true
# Whether to unfreeze description model (case descriptions)
UNFREEZE_DESCRIPTION_MODEL=true

# Random Seeds
# -----------
# Random seed for reproducibility
RANDOM_SEED=42
# PyTorch random seed
TORCH_SEED=42
# NumPy random seed
NUMPY_SEED=42

# Model Loading and Caching
# -------------------------
# Whether to use local cache for sentence transformer models
USE_MODEL_CACHE=true
# Whether to download models if not found locally
DOWNLOAD_MODELS=true

# Data Validation
# --------------
# Minimum number of justices required per case
MIN_JUSTICES_PER_CASE=1
# Maximum number of justices allowed per case
MAX_JUSTICES_PER_CASE=15
# Whether to skip cases with missing descriptions
SKIP_MISSING_DESCRIPTIONS=true
# Whether to skip cases with no valid justice biographies
SKIP_MISSING_BIOGRAPHIES=true

# Hyperparameter Optimization
# ---------------------------
# Number of optimization trials to run
OPTUNA_N_TRIALS=50
# Maximum time per trial in seconds (0 for no limit)
OPTUNA_MAX_TRIAL_TIME=300
# Maximum epochs per trial during optimization
OPTUNA_MAX_EPOCHS=5
# Minimum epochs before early stopping can occur
OPTUNA_MIN_EPOCHS=2
# Early stopping patience for optimization trials
OPTUNA_EARLY_STOP_PATIENCE=2
# Maximum training samples per trial (0 for no limit)
OPTUNA_MAX_TRAIN_SAMPLES=500
# Maximum validation samples per trial (0 for no limit)
OPTUNA_MAX_VAL_SAMPLES=100
# Optuna pruner startup trials
OPTUNA_PRUNER_STARTUP_TRIALS=5
# Optuna pruner warmup steps
OPTUNA_PRUNER_WARMUP_STEPS=3

# Hyperparameter Tuning Control
# -----------------------------
# Controls which hyperparameters to optimize during hyperparameter tuning.
# Set to 'true' to allow Optuna to suggest values for the parameter.
# Set to 'false' to use the default value defined above in this config file.
#
# This allows you to selectively tune only certain parameters while keeping
# others fixed to their default values, which can be useful for:
# - Focusing optimization on specific aspects of the model
# - Reducing optimization time by tuning fewer parameters
# - Comparing the effect of specific hyperparameters
#
# Example: To only tune learning rate and dropout while keeping architecture fixed:
#   TUNE_LEARNING_RATE=true
#   TUNE_DROPOUT_RATE=true  
#   TUNE_HIDDEN_DIM=false
#   TUNE_NUM_ATTENTION_HEADS=false
#   etc.

# Model Architecture Parameters
TUNE_HIDDEN_DIM=true                    # Tune hidden layer dimensions
TUNE_NUM_ATTENTION_HEADS=true           # Tune number of attention heads
TUNE_USE_JUSTICE_ATTENTION=true         # Tune whether to use justice attention

# Regularization Parameters  
TUNE_DROPOUT_RATE=true                  # Tune dropout rate
TUNE_WEIGHT_DECAY=true                  # Tune weight decay (L2 regularization)

# Training Parameters
TUNE_LEARNING_RATE=true                 # Tune learning rate
TUNE_BATCH_SIZE=true                    # Tune batch size

# Sentence Transformer Fine-tuning Control
TUNE_UNFREEZE_EPOCH=true                # Tune when to unfreeze sentence transformers

# Hyperparameter Search Spaces
# ----------------------------
# Hidden dimension options (comma-separated)
OPTUNA_HIDDEN_DIM_OPTIONS=256,512,768,1024
# Dropout rate range (min,max,step)
OPTUNA_DROPOUT_RATE_RANGE=0.1,0.5,0.1
# Number of attention heads options (comma-separated)
OPTUNA_ATTENTION_HEADS_OPTIONS=4,6,8,16,32
# Justice attention options (comma-separated boolean)
OPTUNA_JUSTICE_ATTENTION_OPTIONS=true,false
# Learning rate range (min,max,log_scale)
OPTUNA_LEARNING_RATE_RANGE=1e-5,1e-3,true
# Batch size options (comma-separated)
OPTUNA_BATCH_SIZE_OPTIONS=8,16
# Weight decay range (min,max,log_scale)
OPTUNA_WEIGHT_DECAY_RANGE=1e-4,1e-1,true
# Unfreeze epoch options (comma-separated integers, -1 means never unfreeze)
OPTUNA_UNFREEZE_EPOCH_OPTIONS=-1,0,1,2,3,5 