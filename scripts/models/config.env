# SCOTUS AI Model Configuration
# =============================
# Centralized hyperparameters for model training and architecture

# Model Architecture
# ------------------
# Sentence transformer model for justice biographies
BIO_MODEL_NAME=sentence-transformers/all-roberta-large-v1
# Sentence transformer model for case descriptions (legal domain)
DESCRIPTION_MODEL_NAME=sentence-transformers/all-roberta-large-v1

# Embedding dimensions (should match sentence transformer output)
EMBEDDING_DIM=384
# Hidden layer dimension for fully connected layers
HIDDEN_DIM=256
# Maximum number of justices that can be on the court
MAX_JUSTICES=11

# Attention Mechanism
# ------------------
# Whether to use justice cross-attention or simple concatenation
USE_JUSTICE_ATTENTION=true
# Number of attention heads for justice cross-attention
NUM_ATTENTION_HEADS=4

# Regularization
# --------------
# Dropout rate for regularization
DROPOUT_RATE=0.2
# Weight decay for optimizer
WEIGHT_DECAY=1e-05
# Use NEFTune regularization
USE_NOISE_REG=true
# NEFTune's Alpha scaler
NOISE_REG_ALPHA=0.45
# Max grad norm
MAX_GRAD_NORM=0.72

# Training Configuration
# ---------------------
# Learning rate for optimizer
LEARNING_RATE=5.5e-05
# Number of training epochs
NUM_EPOCHS=50
# Batch size for training
BATCH_SIZE=12
# Number of data loader workers
NUM_WORKERS=4

# Early Stopping and Scheduling
# -----------------------------
# Patience for early stopping (epochs without improvement)
PATIENCE=10

# Device Configuration
# -------------------
# Options: cuda, cpu, auto (auto will detect available device)
DEVICE=auto

# Data Paths
# ----------
# Path to case dataset JSON file
DATASET_FILE=data/processed/case_dataset.json
# Path to pre-tokenized biography data
BIO_TOKENIZED_FILE=data/processed/encoded_bios.pkl
# Path to pre-tokenized case description data
DESCRIPTION_TOKENIZED_FILE=data/processed/encoded_descriptions.pkl

# Output Paths
# -----------
# Directory to save trained models
MODEL_OUTPUT_DIR=models_output
# Best model filename
BEST_MODEL_NAME=best_model.pth

# Dataset Splitting
# ----------------
# Training set ratio
TRAIN_RATIO=0.8
# Validation set ratio
VAL_RATIO=0.2
# Test set ratio
TEST_RATIO=0.1
# Random seed for dataset splitting
SPLIT_RANDOM_STATE=42

# Sentence Transformer Fine-tuning Strategy
# ----------------------------------------
# Whether to unfreeze bio model (justice biographies)
UNFREEZE_BIO_MODEL=true
# Whether to unfreeze description model (case descriptions)
UNFREEZE_DESCRIPTION_MODEL=true
# Epoch to start unfreezing sentence transformers
UNFREEZE_AT_EPOCH=6
# Learning rate for sentence transformer fine-tuning
SENTENCE_TRANSFORMER_LEARNING_RATE=2e-05

# Pretrained Model Path
# --------------------
# Path to pretrained bio model (optional)
# PRETRAINED_BIO_MODEL="scotus_ai/models_output/contrastive_justice/best_0808_089_035"
PRETRAINED_BIO_MODEL=""

# =============================================================================
# HYPERPARAMETER OPTIMIZATION CONFIGURATION
# =============================================================================

# Study Configuration
# -------------------
# Number of optimization trials to run
OPTUNA_N_TRIALS=75
# Study name for Optuna optimization
OPTUNA_STUDY_NAME=scotus_voting_model_optimization
# Maximum time per trial in seconds (0 for no limit)
OPTUNA_MAX_TRIAL_TIME=3000
# Maximum epochs per trial during optimization
OPTUNA_MAX_EPOCHS=10
# Minimum epochs before early stopping can occur
OPTUNA_MIN_EPOCHS=4
# Early stopping patience for optimization trials
OPTUNA_EARLY_STOP_PATIENCE=10

# Hyperparameter Tuning Control
# -----------------------------
# Controls which hyperparameters to optimize during hyperparameter tuning.
# Set to 'true' to allow Optuna to suggest values for the parameter.
# Set to 'false' to use the default value defined above in this config file.

# Model Architecture Control
TUNE_HIDDEN_DIM=false
TUNE_NUM_ATTENTION_HEADS=false
TUNE_USE_JUSTICE_ATTENTION=false

# Training Parameters Control
TUNE_LEARNING_RATE=true
TUNE_BATCH_SIZE=false
TUNE_WEIGHT_DECAY=true
TUNE_MAX_GRAD_NORM=true

# Regularization Control
TUNE_DROPOUT_RATE=false
TUNE_USE_NOISE_REG=true

# Unfreezing Strategy Control
TUNE_UNFREEZING=false

# Pretrained Model Control
TUNE_PRETRAINED_BIO_MODEL=false

# Hyperparameter Search Spaces
# ----------------------------
# Hidden dimension options (comma-separated)
OPTUNA_HIDDEN_DIM_OPTIONS=256,512,768,1024
# Number of attention heads options (comma-separated)
OPTUNA_ATTENTION_HEADS_OPTIONS=2,4,6,8
# Justice attention options (comma-separated)
OPTUNA_JUSTICE_ATTENTION_OPTIONS=true,false
# Batch size options (comma-separated)
OPTUNA_BATCH_SIZE_OPTIONS=8,16,32
# Learning rate range (min,max,log_scale)
OPTUNA_LEARNING_RATE_RANGE=4.8e-5,6e-5,true
# Weight decay range (min,max,log_scale)
OPTUNA_WEIGHT_DECAY_RANGE=1e-5,2e-5,true
# Dropout rate range (min,max,step)
OPTUNA_DROPOUT_RATE_RANGE=0.0,0.5,0.1
# Use noise regularization options (comma-separated)
OPTUNA_USE_NOISE_REG_OPTIONS=true,false
# Noise regularization alpha range (min,max,log_scale)
OPTUNA_NOISE_REG_ALPHA_RANGE=0.1,0.5,false
# Unfreeze at epoch options (comma-separated)
OPTUNA_UNFREEZE_AT_EPOCH_OPTIONS=1,2,4,6
# Sentence transformer learning rate range (min,max,log_scale)
OPTUNA_SENTENCE_TRANSFORMER_LR_RANGE=1e-5,4e-5,true
# Max grad norm range
OPTUNA_MAX_GRAD_NORM_RANGE=0.5,0.8,false
# Pretrained bio model options (comma-separated, empty string means no pretrained model)
OPTUNA_PRETRAINED_BIO_MODEL_OPTIONS=,scotus_ai/models_output/contrastive_justice/best_0808_089_035

# Time-Based Cross-Validation Configuration
# -----------------------------------------
# Use time-based cross-validation for hyperparameter optimization
USE_TIME_BASED_CV=true
# Number of time-based CV folds (each fold trains and validates on different time periods)
TIME_BASED_CV_FOLDS=2
# Size of training set for each CV fold (number of cases)
TIME_BASED_CV_TRAIN_SIZE=700
# Size of validation set for each CV fold (number of cases)
TIME_BASED_CV_VAL_SIZE=300