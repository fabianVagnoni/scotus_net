# Contrastive Justice Pretraining

This module implements a contrastive learning framework to pretrain a sophisticated sentence encoder on the biographies of Supreme Court justices. The primary objective is to develop an embedding model that can discern a justice's underlying judicial philosophy and predictive signals from their career history *before* their appointment to the Supreme Court.

This is achieved using a teacher-student architecture where the model learns to align the representation of a justice's pre-confirmation biography with the representation of their complete life story, which includes their tenure on the Court.

## üéØ Core Concept

The fundamental hypothesis of this pretraining task is that a justice's career path, experiences, and writings prior to their Supreme Court confirmation contain latent signals that are predictive of their future judicial behavior.

To capture these signals, we train a **student encoder** on "truncated" biographies (containing only pre-confirmation information). The student is tasked with producing an embedding that is highly similar to the embedding generated by a frozen **teacher encoder** from the justice's "full" biography.

By learning to bridge this informational gap, the student encoder is forced to identify and encode the subtle yet crucial elements in a justice's early life that correlate with their eventual judicial record.

## üèóÔ∏è Architecture

The pretraining architecture is based on a teacher-student model, utilizing the `SentenceTransformer` library. The entire process is designed to optimize the student encoder for the downstream task of vote prediction.

![Contrastive Justice Model Architecture](https://storage.googleapis.com/agent-tools-prod.appspot.com/tool-results/v1/files/944e8d87-9b2f-488f-a9ac-742a03c27e85)

**1. Model Components (`constrastive_justice.py`)**
-   **Student Encoder**: A `SentenceTransformer` model (e.g., `all-roberta-large-v1`) that processes the truncated biographies. Its weights are fine-tuned during training.
-   **Teacher Encoder**: An identical, deep-copied `SentenceTransformer` model that processes the full biographies. Its weights are **frozen**, providing a stable and reliable target representation.

**2. Contrastive Loss Function (`loss.py`)**
The training objective is driven by a composite loss function, combining two distinct measures of similarity, weighted by a hyperparameter `Œ±`:

\[ \text{Loss} = \alpha \cdot \text{NT-XentLoss}(e_t, e_f) + (1 - \alpha) \cdot \text{MSE}(e_t, e_f) \]

-   **NT-Xent Loss**: The primary contrastive component, adapted from SimCLR. It encourages the embeddings of a positive pair (truncated and full biographies of the same justice) to be more similar than embeddings of negative pairs (different justices within the same batch). The `temperature` parameter controls the sharpness of the similarity distribution.
-   **MSE Loss**: A secondary regularization component that directly minimizes the Mean Squared Error between the student's embedding (`e_t`) and the teacher's embedding (`e_f`) for a positive pair. This encourages the student to directly mimic the teacher's output.

## üìä Data & Temporal Splitting

The integrity of the model's evaluation hinges on a carefully designed temporal data splitting strategy.

**1. Input Data**
The pretraining pipeline requires three key data assets, generated by the [Data Pipeline](../data_pipeline/README.md):
-   **Justices Metadata (`data/raw/justices.json`)**: Contains biographical information for each justice, including their crucial `appointment_date`.
-   **Truncated Biographies (`data/processed/encoded_pre_trunc_bios.pkl`)**: Tokenized biographies containing only information from *before* a justice's Supreme Court appointment.
-   **Full Biographies (`data/processed/encoded_pre_full_bios.pkl`)**: Tokenized biographies containing the complete life history of each justice.

**2. Temporal Splitting Strategy**
To simulate a realistic historical prediction scenario, the dataset is split chronologically based on each justice's appointment year.
-   **Training Set**: Comprises justices appointed in earlier historical periods.
-   **Validation & Test Sets**: Comprise justices appointed more recently.

This method ensures that the model is evaluated on its ability to generalize to new, unseen justices from future time periods, providing a robust measure of its predictive power and preventing data leakage from the future. The sizes of the validation and test sets are controlled by `VAL_SET_SIZE` and `TEST_SET_SIZE` in the configuration.

## üìà Training Process (`contrastive_trainer.py`)

The training is orchestrated by the `ContrastiveJusticeTrainer` class, which manages the entire end-to-end process.

-   **Optimizer**: The model uses the `AdamW` optimizer, which is well-suited for transformer-based models.
-   **Learning Rate Scheduling**: A `ReduceLROnPlateau` scheduler dynamically adjusts the learning rate. It monitors the validation loss and reduces the learning rate if no improvement is observed for a specified number of epochs (`LR_SCHEDULER_PATIENCE`).
-   **Early Stopping**: To prevent overfitting and save computational resources, training automatically halts if the validation loss fails to improve for a `MAX_PATIENCE` number of epochs.
-   **Evaluation Metric**: While validation loss guides the learning rate and checkpointing, the primary metric for evaluating hyperparameter configurations is **Mean Reciprocal Rank (MRR)**. MRR assesses how effectively the student model can identify the correct full biography embedding for a given truncated biography embedding from a pool of candidates.

## Hyperparameter Optimization (`hyperparameter_tuning.py`)

The framework includes a powerful hyperparameter tuning script that leverages **Optuna** to systematically search for the optimal set of hyperparameters.

-   **Objective**: The tuner's goal is to **maximize the MRR** on the validation set.
-   **Pruning**: A `MedianPruner` is employed to intelligently terminate unpromising trials early, making the search process highly efficient.
-   **Tuned Parameters**: The search space is configurable but typically includes:
    -   `learning_rate`
    -   `batch_size`
    -   `weight_decay`
    -   `dropout_rate`
    -   `temperature` (from NT-Xent loss)
    -   `alpha` (loss component weighting)

## üöÄ How to Run

Execution is managed through simple command-line scripts.

**1. Configuration (`config.py`, `config.env`)**
All training and tuning parameters are centralized in `scripts/pretraining/config.env`. Before running, you can customize key settings:
```bash
# Model, training, and loss parameters
MODEL_NAME=sentence-transformers/all-roberta-large-v1
BATCH_SIZE=4
LEARNING_RATE=5e-5
NUM_EPOCHS=50
TEMPERATURE=0.07
ALPHA=0.7

# Data split configuration
TEST_SET_SIZE=20
VAL_SET_SIZE=20

# Output directory for the trained model
MODEL_OUTPUT_DIR=models/contrastive_justice
```

**2. Run Standard Training**
To run the pretraining process with the parameters defined in `config.env`:
```bash
python scripts/pretraining/run_training.py
```

**3. Run Hyperparameter Tuning**
To initiate an Optuna study, provide a unique name for the experiment. Results and logs will be saved accordingly.
```bash
python scripts/pretraining/hyperparameter_tuning.py --experiment-name "roberta_large_tuning_v1" --n-trials 100
```

## üíæ Model Outputs

The training process saves the best-performing student encoder based on validation loss.

-   **Output Directory**: The final model artifacts are saved in the directory specified by `MODEL_OUTPUT_DIR` (e.g., `models/contrastive_justice/`).
-   **Saved Model**: The trainer saves the entire `SentenceTransformer` student model to a `best_model` subdirectory. This allows for easy loading for inference or further fine-tuning in downstream tasks.
-   **Logs**: A detailed log of the training process, including loss and metrics for each epoch, is also generated. For hyperparameter tuning, detailed logs for each trial are stored in `logs/hyperparameter_tunning_logs/`.

This pretrained encoder is a critical component for the final SCOTUS vote prediction model, providing it with a rich, nuanced understanding of each justice's background.

## üìö References

-   **Contrastive Learning (SimCLR)**: Chen, T., Kornblith, S., Norouzi, M., & Hinton, G. (2020). *A Simple Framework for Contrastive Learning of Visual Representations*. [arXiv:2002.05709](https://arxiv.org/abs/2002.05709).
-   **Teacher-Student Knowledge Distillation**: Hinton, G., Vinyals, O., & Dean, J. (2015). *Distilling the Knowledge in a Neural Network*. [arXiv:1503.02531](https://arxiv.org/abs/1503.02531).
-   **Sentence-Transformers**: Reimers, N., & Gurevych, I. (2019). *Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks*. [arXiv:1908.10084](https://arxiv.org/abs/1908.10084).
-   **Optuna**: Akiba, T., Sano, S., Yanase, T., Ohta, T., & Koyama, M. (2019). *Optuna: A Next-generation Hyperparameter Optimization Framework*. [arXiv:1907.10902](https://arxiv.org/abs/1907.10902).
-   **AdamW Optimizer**: Loshchilov, I., & Hutter, F. (2017). *Decoupled Weight Decay Regularization*. [arXiv:1711.05101](https://arxiv.org/abs/1711.05101). 